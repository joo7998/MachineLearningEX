{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09eb397c-d8b8-471c-abb3-41ef0de28faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from sklearn.model_selection import train_test_split\n",
    " \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34498704-4af8-4173-97d3-dea753bb2e23",
   "metadata": {},
   "source": [
    "# 선형 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "18505406-707c-46b0-b45b-dfc51225cdc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../dataset/housing.csv', delim_whitespace=True, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f6f7340-6419-4eff-bb4d-c0ac22a030ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 506 entries, 0 to 505\n",
      "Data columns (total 14 columns):\n",
      " #   Column  Non-Null Count  Dtype  \n",
      "---  ------  --------------  -----  \n",
      " 0   0       506 non-null    float64\n",
      " 1   1       506 non-null    float64\n",
      " 2   2       506 non-null    float64\n",
      " 3   3       506 non-null    int64  \n",
      " 4   4       506 non-null    float64\n",
      " 5   5       506 non-null    float64\n",
      " 6   6       506 non-null    float64\n",
      " 7   7       506 non-null    float64\n",
      " 8   8       506 non-null    int64  \n",
      " 9   9       506 non-null    float64\n",
      " 10  10      506 non-null    float64\n",
      " 11  11      506 non-null    float64\n",
      " 12  12      506 non-null    float64\n",
      " 13  13      506 non-null    float64\n",
      "dtypes: float64(12), int64(2)\n",
      "memory usage: 55.5 KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e121b1c-92ca-4a88-9803-ca6902365cae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "      <td>24.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "      <td>21.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242.0</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "      <td>34.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "      <td>33.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222.0</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "      <td>36.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        0     1     2   3      4      5     6       7   8      9     10  \\\n",
       "0  0.00632  18.0  2.31   0  0.538  6.575  65.2  4.0900   1  296.0  15.3   \n",
       "1  0.02731   0.0  7.07   0  0.469  6.421  78.9  4.9671   2  242.0  17.8   \n",
       "2  0.02729   0.0  7.07   0  0.469  7.185  61.1  4.9671   2  242.0  17.8   \n",
       "3  0.03237   0.0  2.18   0  0.458  6.998  45.8  6.0622   3  222.0  18.7   \n",
       "4  0.06905   0.0  2.18   0  0.458  7.147  54.2  6.0622   3  222.0  18.7   \n",
       "\n",
       "       11    12    13  \n",
       "0  396.90  4.98  24.0  \n",
       "1  396.90  9.14  21.6  \n",
       "2  392.83  4.03  34.7  \n",
       "3  394.63  2.94  33.4  \n",
       "4  396.90  5.33  36.2  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33e7abb7-0438-4b5c-8b7e-b92f9544c489",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 샘플의 수는 506\n",
    "# 13개의 속성, 1개의 클래스\n",
    "# 13th : 1000$ (단위)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec379f40-ef95-4fac-b311-dfe35cad9310",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(2)\n",
    "# seed 3 => 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9d006a58-8a20-4e8d-b4d0-26d3948eb5a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df.values\n",
    "X = dataset[:,0:13].astype(float)\n",
    "#astype(float)\n",
    "Y = dataset[:,13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a325f72-5290-4237-a390-b1e2f28b27ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.3, random_state=seed)\n",
    "\n",
    "# random_state=seed 빼기 ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bc79d1d-ea25-476c-a489-068e6705b0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(30, input_dim=13, activation='relu'))\n",
    "model.add(Dense(6, activation='relu'))\n",
    "model.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33e7b955-8c24-4fc5-ace2-44f3f550c5ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='mean_squared_error', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffb22497-8f93-4df1-a810-d5ca055ec4a3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "36/36 [==============================] - 0s 694us/step - loss: 21136.7520\n",
      "Epoch 2/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 737.8435\n",
      "Epoch 3/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 162.0087\n",
      "Epoch 4/200\n",
      "36/36 [==============================] - 0s 611us/step - loss: 124.7283\n",
      "Epoch 5/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 113.6169\n",
      "Epoch 6/200\n",
      "36/36 [==============================] - 0s 611us/step - loss: 105.3638\n",
      "Epoch 7/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 97.4607\n",
      "Epoch 8/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 92.0170\n",
      "Epoch 9/200\n",
      "36/36 [==============================] - 0s 611us/step - loss: 89.1127\n",
      "Epoch 10/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 88.2072\n",
      "Epoch 11/200\n",
      "36/36 [==============================] - 0s 666us/step - loss: 82.7861\n",
      "Epoch 12/200\n",
      "36/36 [==============================] - 0s 778us/step - loss: 80.8981\n",
      "Epoch 13/200\n",
      "36/36 [==============================] - 0s 611us/step - loss: 78.8906\n",
      "Epoch 14/200\n",
      "36/36 [==============================] - 0s 611us/step - loss: 76.2699\n",
      "Epoch 15/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 75.2026\n",
      "Epoch 16/200\n",
      "36/36 [==============================] - 0s 611us/step - loss: 73.6337\n",
      "Epoch 17/200\n",
      "36/36 [==============================] - 0s 611us/step - loss: 72.7286\n",
      "Epoch 18/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 71.3474\n",
      "Epoch 19/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 70.0454\n",
      "Epoch 20/200\n",
      "36/36 [==============================] - 0s 611us/step - loss: 69.2973\n",
      "Epoch 21/200\n",
      "36/36 [==============================] - 0s 583us/step - loss: 66.8574\n",
      "Epoch 22/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 68.1678\n",
      "Epoch 23/200\n",
      "36/36 [==============================] - 0s 611us/step - loss: 64.6428\n",
      "Epoch 24/200\n",
      "36/36 [==============================] - 0s 611us/step - loss: 64.3983\n",
      "Epoch 25/200\n",
      "36/36 [==============================] - 0s 765us/step - loss: 62.2085\n",
      "Epoch 26/200\n",
      "36/36 [==============================] - 0s 611us/step - loss: 62.4244\n",
      "Epoch 27/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 61.1320\n",
      "Epoch 28/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 60.5302\n",
      "Epoch 29/200\n",
      "36/36 [==============================] - 0s 694us/step - loss: 58.7072\n",
      "Epoch 30/200\n",
      "36/36 [==============================] - 0s 694us/step - loss: 58.2254\n",
      "Epoch 31/200\n",
      "36/36 [==============================] - 0s 695us/step - loss: 57.6955\n",
      "Epoch 32/200\n",
      "36/36 [==============================] - 0s 695us/step - loss: 56.6812\n",
      "Epoch 33/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 55.2349\n",
      "Epoch 34/200\n",
      "36/36 [==============================] - 0s 722us/step - loss: 55.3518\n",
      "Epoch 35/200\n",
      "36/36 [==============================] - 0s 778us/step - loss: 54.6175\n",
      "Epoch 36/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 53.7413\n",
      "Epoch 37/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 52.2852\n",
      "Epoch 38/200\n",
      "36/36 [==============================] - 0s 778us/step - loss: 52.5918\n",
      "Epoch 39/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 51.9095\n",
      "Epoch 40/200\n",
      "36/36 [==============================] - 0s 750us/step - loss: 50.6746\n",
      "Epoch 41/200\n",
      "36/36 [==============================] - 0s 694us/step - loss: 50.2417\n",
      "Epoch 42/200\n",
      "36/36 [==============================] - 0s 722us/step - loss: 49.2314\n",
      "Epoch 43/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 51.0042\n",
      "Epoch 44/200\n",
      "36/36 [==============================] - 0s 611us/step - loss: 48.5258\n",
      "Epoch 45/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 47.9635\n",
      "Epoch 46/200\n",
      "36/36 [==============================] - 0s 722us/step - loss: 47.4356\n",
      "Epoch 47/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 45.9358\n",
      "Epoch 48/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 48.1263\n",
      "Epoch 49/200\n",
      "36/36 [==============================] - 0s 611us/step - loss: 45.3924\n",
      "Epoch 50/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 47.2404\n",
      "Epoch 51/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 45.1570\n",
      "Epoch 52/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 44.5927\n",
      "Epoch 53/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 44.8504\n",
      "Epoch 54/200\n",
      "36/36 [==============================] - 0s 694us/step - loss: 45.6176\n",
      "Epoch 55/200\n",
      "36/36 [==============================] - 0s 723us/step - loss: 43.2040\n",
      "Epoch 56/200\n",
      "36/36 [==============================] - 0s 722us/step - loss: 42.3511\n",
      "Epoch 57/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 41.6741\n",
      "Epoch 58/200\n",
      "36/36 [==============================] - 0s 722us/step - loss: 40.9511\n",
      "Epoch 59/200\n",
      "36/36 [==============================] - 0s 694us/step - loss: 40.0339\n",
      "Epoch 60/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 41.3358\n",
      "Epoch 61/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 40.4423\n",
      "Epoch 62/200\n",
      "36/36 [==============================] - 0s 611us/step - loss: 39.2869\n",
      "Epoch 63/200\n",
      "36/36 [==============================] - 0s 611us/step - loss: 39.9539\n",
      "Epoch 64/200\n",
      "36/36 [==============================] - 0s 611us/step - loss: 39.4840\n",
      "Epoch 65/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 39.7725\n",
      "Epoch 66/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 40.3317\n",
      "Epoch 67/200\n",
      "36/36 [==============================] - 0s 694us/step - loss: 38.8742\n",
      "Epoch 68/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 37.6335\n",
      "Epoch 69/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 38.1220\n",
      "Epoch 70/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 37.5524\n",
      "Epoch 71/200\n",
      "36/36 [==============================] - 0s 611us/step - loss: 36.9016\n",
      "Epoch 72/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 37.7491\n",
      "Epoch 73/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 38.2591\n",
      "Epoch 74/200\n",
      "36/36 [==============================] - 0s 611us/step - loss: 38.6238\n",
      "Epoch 75/200\n",
      "36/36 [==============================] - 0s 611us/step - loss: 36.2643\n",
      "Epoch 76/200\n",
      "36/36 [==============================] - 0s 611us/step - loss: 35.1660\n",
      "Epoch 77/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 37.4754\n",
      "Epoch 78/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 36.5087\n",
      "Epoch 79/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 35.1409\n",
      "Epoch 80/200\n",
      "36/36 [==============================] - 0s 611us/step - loss: 37.1743\n",
      "Epoch 81/200\n",
      "36/36 [==============================] - 0s 611us/step - loss: 34.5440\n",
      "Epoch 82/200\n",
      "36/36 [==============================] - 0s 611us/step - loss: 35.5552\n",
      "Epoch 83/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 36.8628\n",
      "Epoch 84/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 33.8898\n",
      "Epoch 85/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 32.8386\n",
      "Epoch 86/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 33.5914\n",
      "Epoch 87/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 33.3402\n",
      "Epoch 88/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 33.9674\n",
      "Epoch 89/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 33.4099\n",
      "Epoch 90/200\n",
      "36/36 [==============================] - 0s 694us/step - loss: 32.9702\n",
      "Epoch 91/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 32.3191\n",
      "Epoch 92/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 33.0260\n",
      "Epoch 93/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 35.0158\n",
      "Epoch 94/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 32.5389\n",
      "Epoch 95/200\n",
      "36/36 [==============================] - 0s 666us/step - loss: 33.0083\n",
      "Epoch 96/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 32.4334\n",
      "Epoch 97/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 31.7788\n",
      "Epoch 98/200\n",
      "36/36 [==============================] - 0s 612us/step - loss: 32.1338\n",
      "Epoch 99/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 31.8812\n",
      "Epoch 100/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 32.0656\n",
      "Epoch 101/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 33.8288\n",
      "Epoch 102/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 32.9395\n",
      "Epoch 103/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 36.5395\n",
      "Epoch 104/200\n",
      "36/36 [==============================] - 0s 611us/step - loss: 32.2947\n",
      "Epoch 105/200\n",
      "36/36 [==============================] - 0s 611us/step - loss: 31.0525\n",
      "Epoch 106/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 30.6595\n",
      "Epoch 107/200\n",
      "36/36 [==============================] - 0s 778us/step - loss: 30.2280\n",
      "Epoch 108/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 31.2667\n",
      "Epoch 109/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 31.4916\n",
      "Epoch 110/200\n",
      "36/36 [==============================] - 0s 750us/step - loss: 32.3186\n",
      "Epoch 111/200\n",
      "36/36 [==============================] - 0s 694us/step - loss: 31.3733\n",
      "Epoch 112/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 30.0707\n",
      "Epoch 113/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 31.5046\n",
      "Epoch 114/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 31.1011\n",
      "Epoch 115/200\n",
      "36/36 [==============================] - 0s 778us/step - loss: 30.1980\n",
      "Epoch 116/200\n",
      "36/36 [==============================] - 0s 611us/step - loss: 29.9912\n",
      "Epoch 117/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 30.4793\n",
      "Epoch 118/200\n",
      "36/36 [==============================] - 0s 611us/step - loss: 32.0282\n",
      "Epoch 119/200\n",
      "36/36 [==============================] - 0s 694us/step - loss: 31.6076\n",
      "Epoch 120/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 30.9727\n",
      "Epoch 121/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 31.4114\n",
      "Epoch 122/200\n",
      "36/36 [==============================] - 0s 722us/step - loss: 30.9190\n",
      "Epoch 123/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 29.3753\n",
      "Epoch 124/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 29.9718\n",
      "Epoch 125/200\n",
      "36/36 [==============================] - 0s 778us/step - loss: 30.9183\n",
      "Epoch 126/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 33.6446\n",
      "Epoch 127/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 29.9245\n",
      "Epoch 128/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 31.3726\n",
      "Epoch 129/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 32.2332\n",
      "Epoch 130/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 28.2083\n",
      "Epoch 131/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 29.0223\n",
      "Epoch 132/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 34.6853\n",
      "Epoch 133/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 30.1644\n",
      "Epoch 134/200\n",
      "36/36 [==============================] - 0s 611us/step - loss: 29.6993\n",
      "Epoch 135/200\n",
      "36/36 [==============================] - 0s 611us/step - loss: 28.4549\n",
      "Epoch 136/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 28.9458\n",
      "Epoch 137/200\n",
      "36/36 [==============================] - 0s 695us/step - loss: 28.7676\n",
      "Epoch 138/200\n",
      "36/36 [==============================] - 0s 694us/step - loss: 32.4170\n",
      "Epoch 139/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 28.0234\n",
      "Epoch 140/200\n",
      "36/36 [==============================] - 0s 611us/step - loss: 29.2666\n",
      "Epoch 141/200\n",
      "36/36 [==============================] - 0s 695us/step - loss: 29.7154\n",
      "Epoch 142/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 33.7544\n",
      "Epoch 143/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 29.3938\n",
      "Epoch 144/200\n",
      "36/36 [==============================] - 0s 722us/step - loss: 28.7050\n",
      "Epoch 145/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 28.9161\n",
      "Epoch 146/200\n",
      "36/36 [==============================] - 0s 694us/step - loss: 28.8767\n",
      "Epoch 147/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 30.6159\n",
      "Epoch 148/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 29.1248\n",
      "Epoch 149/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 32.7024\n",
      "Epoch 150/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 27.5032\n",
      "Epoch 151/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 28.2352\n",
      "Epoch 152/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 27.7181\n",
      "Epoch 153/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 27.5111\n",
      "Epoch 154/200\n",
      "36/36 [==============================] - 0s 750us/step - loss: 28.8709\n",
      "Epoch 155/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 28.6613\n",
      "Epoch 156/200\n",
      "36/36 [==============================] - 0s 666us/step - loss: 30.3630\n",
      "Epoch 157/200\n",
      "36/36 [==============================] - 0s 722us/step - loss: 29.7454\n",
      "Epoch 158/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 28.7471\n",
      "Epoch 159/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 31.7824\n",
      "Epoch 160/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 29.1583\n",
      "Epoch 161/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 27.1048\n",
      "Epoch 162/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 28.9372\n",
      "Epoch 163/200\n",
      "36/36 [==============================] - 0s 722us/step - loss: 27.2985\n",
      "Epoch 164/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 29.0137\n",
      "Epoch 165/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 29.9140\n",
      "Epoch 166/200\n",
      "36/36 [==============================] - 0s 694us/step - loss: 31.1387\n",
      "Epoch 167/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 27.9005\n",
      "Epoch 168/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 32.8941\n",
      "Epoch 169/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 28.0838\n",
      "Epoch 170/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 27.3544\n",
      "Epoch 171/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 26.8106\n",
      "Epoch 172/200\n",
      "36/36 [==============================] - 0s 778us/step - loss: 28.3993\n",
      "Epoch 173/200\n",
      "36/36 [==============================] - 0s 666us/step - loss: 26.9667\n",
      "Epoch 174/200\n",
      "36/36 [==============================] - 0s 778us/step - loss: 26.5496\n",
      "Epoch 175/200\n",
      "36/36 [==============================] - 0s 750us/step - loss: 27.2569\n",
      "Epoch 176/200\n",
      "36/36 [==============================] - 0s 694us/step - loss: 27.0909\n",
      "Epoch 177/200\n",
      "36/36 [==============================] - 0s 805us/step - loss: 26.7207\n",
      "Epoch 178/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 25.5782\n",
      "Epoch 179/200\n",
      "36/36 [==============================] - 0s 889us/step - loss: 28.4325\n",
      "Epoch 180/200\n",
      "36/36 [==============================] - 0s 694us/step - loss: 27.7505\n",
      "Epoch 181/200\n",
      "36/36 [==============================] - 0s 694us/step - loss: 28.0197\n",
      "Epoch 182/200\n",
      "36/36 [==============================] - 0s 722us/step - loss: 30.0381\n",
      "Epoch 183/200\n",
      "36/36 [==============================] - 0s 695us/step - loss: 32.0801\n",
      "Epoch 184/200\n",
      "36/36 [==============================] - 0s 722us/step - loss: 26.6037\n",
      "Epoch 185/200\n",
      "36/36 [==============================] - 0s 694us/step - loss: 25.6245\n",
      "Epoch 186/200\n",
      "36/36 [==============================] - 0s 833us/step - loss: 27.6351\n",
      "Epoch 187/200\n",
      "36/36 [==============================] - 0s 750us/step - loss: 25.3317\n",
      "Epoch 188/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 25.2909\n",
      "Epoch 189/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 29.0548\n",
      "Epoch 190/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 26.5733\n",
      "Epoch 191/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 24.9651\n",
      "Epoch 192/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 25.1889\n",
      "Epoch 193/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 26.9459\n",
      "Epoch 194/200\n",
      "36/36 [==============================] - 0s 722us/step - loss: 26.6908\n",
      "Epoch 195/200\n",
      "36/36 [==============================] - 0s 611us/step - loss: 25.7008\n",
      "Epoch 196/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 25.5819\n",
      "Epoch 197/200\n",
      "36/36 [==============================] - 0s 778us/step - loss: 25.6698\n",
      "Epoch 198/200\n",
      "36/36 [==============================] - 0s 667us/step - loss: 27.9447\n",
      "Epoch 199/200\n",
      "36/36 [==============================] - 0s 778us/step - loss: 27.1158\n",
      "Epoch 200/200\n",
      "36/36 [==============================] - 0s 639us/step - loss: 25.1105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1ef2d26e848>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, epochs=200, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6292882-84fb-4321-8bc0-4c9974298393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 예측 값과 실제 값의 비교\n",
    "Y_prediction = model.predict(X_test).flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5b7f09d1-1c62-4b9c-8143-c3bcdd9053ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flatten() 쓰는 이유"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d62926b6-b45b-40bf-bdab-77374d14032d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23.668133  26.592363  26.246851  15.164432  20.859827  19.971296\n",
      " 21.350029  23.694624  15.70524   12.34992    7.9843206 15.207081\n",
      " 19.438633   8.072366  41.32915   29.788103  23.23147   34.488277\n",
      " 28.62037   22.574911  22.92856   23.889225  21.007238  26.775179\n",
      " 20.416094  18.885206  16.676405  14.193611  29.239975  21.192158\n",
      " 18.797289  21.044409  20.235384  19.895374  24.989454  23.880869\n",
      " 10.686763  28.52164   17.04995   13.94484   22.89411   17.87525\n",
      " 21.378788  17.633081  24.962633  24.789381  14.751832  22.204437\n",
      " 12.039266  25.296389  14.810805  17.145702  20.329243  30.419312\n",
      " 10.481079  18.060621  18.118902  15.590366  18.787458  19.25615\n",
      " 23.61876   18.770733  30.499031  30.218756  18.468483  30.91313\n",
      " 20.10961   14.729119  17.33966   21.40325   21.268301  23.591782\n",
      " 31.815767  30.415958  26.630085   4.749509  30.246727  19.919893\n",
      " 23.39607   21.423967  23.847666  23.356863  23.270247  31.266718\n",
      " 31.91791   24.689714  20.855038  14.993311  31.10436   17.433891\n",
      " 21.090748  13.548125  25.064735  31.101637  22.600216  21.277224\n",
      "  0.5338683 27.243855  17.03873   17.856245  21.840994  23.792719\n",
      " 28.095459  20.632223  21.376179  19.69621    7.4717684 20.883245\n",
      " 21.646126  26.256329  33.193527  11.915755  16.262651  16.504307\n",
      "  6.623642  18.64348    3.2104928 24.945208  11.604663  46.204376\n",
      " 30.491941  13.875734  16.74769   23.043503  21.184658  18.978683\n",
      " 36.07332   12.450999  20.646307  30.53162   20.616425  11.069927\n",
      " 21.752138  16.4261    12.386297  35.7181    20.860048  18.16534\n",
      " 25.63958    8.064611  13.537732  21.769035  34.013275  26.277617\n",
      " 25.52513   14.514604  31.906466  27.314596  13.405164   7.482919\n",
      " 26.049786  26.553377 ]\n"
     ]
    }
   ],
   "source": [
    "print(Y_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba0a1184-b43d-44e5-849e-f1ea5464b4e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[22.6 50.  23.   8.3 21.2 19.9 20.6 18.7 16.1 18.6  8.8 17.2 14.9 10.5\n",
      " 50.  29.  23.  33.3 29.4 21.  23.8 19.1 20.4 29.1 19.3 23.1 19.6 19.4\n",
      " 38.7 18.7 14.6 20.  20.5 20.1 23.6 16.8  5.6 50.  14.5 13.3 23.9 20.\n",
      " 19.8 13.8 16.5 21.6 20.3 17.  11.8 27.5 15.6 23.1 24.3 42.8 15.6 21.7\n",
      " 17.1 17.2 15.  21.7 18.6 21.  33.1 31.5 20.1 29.8 15.2 15.  27.5 22.6\n",
      " 20.  21.4 23.5 31.2 23.7  7.4 48.3 24.4 22.6 18.3 23.3 17.1 27.9 44.8\n",
      " 50.  23.  21.4 10.2 23.3 23.2 18.9 13.4 21.9 24.8 11.9 24.3 13.8 24.7\n",
      " 14.1 18.7 28.1 19.8 26.7 21.7 22.  22.9 10.4 21.9 20.6 26.4 41.3 17.2\n",
      " 27.1 20.4 16.5 24.4  8.4 23.   9.7 50.  30.5 12.3 19.4 21.2 20.3 18.8\n",
      " 33.4 18.5 19.6 33.2 13.1  7.5 13.6 17.4  8.4 35.4 24.  13.4 26.2  7.2\n",
      " 13.1 24.5 37.2 25.  24.1 16.6 32.9 36.2 11.   7.2 22.8 28.7]\n"
     ]
    }
   ],
   "source": [
    "print(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7998e809-9571-4042-ab70-a75c43c5f445",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "실제가격: 22.600, 예상가격: 23.668\n",
      "실제가격: 50.000, 예상가격: 26.592\n",
      "실제가격: 23.000, 예상가격: 26.247\n",
      "실제가격: 8.300, 예상가격: 15.164\n",
      "실제가격: 21.200, 예상가격: 20.860\n",
      "실제가격: 19.900, 예상가격: 19.971\n",
      "실제가격: 20.600, 예상가격: 21.350\n",
      "실제가격: 18.700, 예상가격: 23.695\n",
      "실제가격: 16.100, 예상가격: 15.705\n",
      "실제가격: 18.600, 예상가격: 12.350\n",
      "실제가격: 8.800, 예상가격: 7.984\n",
      "실제가격: 17.200, 예상가격: 15.207\n",
      "실제가격: 14.900, 예상가격: 19.439\n",
      "실제가격: 10.500, 예상가격: 8.072\n",
      "실제가격: 50.000, 예상가격: 41.329\n",
      "실제가격: 29.000, 예상가격: 29.788\n",
      "실제가격: 23.000, 예상가격: 23.231\n",
      "실제가격: 33.300, 예상가격: 34.488\n",
      "실제가격: 29.400, 예상가격: 28.620\n",
      "실제가격: 21.000, 예상가격: 22.575\n",
      "실제가격: 23.800, 예상가격: 22.929\n",
      "실제가격: 19.100, 예상가격: 23.889\n",
      "실제가격: 20.400, 예상가격: 21.007\n",
      "실제가격: 29.100, 예상가격: 26.775\n",
      "실제가격: 19.300, 예상가격: 20.416\n",
      "실제가격: 23.100, 예상가격: 18.885\n",
      "실제가격: 19.600, 예상가격: 16.676\n",
      "실제가격: 19.400, 예상가격: 14.194\n",
      "실제가격: 38.700, 예상가격: 29.240\n",
      "실제가격: 18.700, 예상가격: 21.192\n",
      "실제가격: 14.600, 예상가격: 18.797\n",
      "실제가격: 20.000, 예상가격: 21.044\n",
      "실제가격: 20.500, 예상가격: 20.235\n",
      "실제가격: 20.100, 예상가격: 19.895\n",
      "실제가격: 23.600, 예상가격: 24.989\n",
      "실제가격: 16.800, 예상가격: 23.881\n",
      "실제가격: 5.600, 예상가격: 10.687\n",
      "실제가격: 50.000, 예상가격: 28.522\n",
      "실제가격: 14.500, 예상가격: 17.050\n",
      "실제가격: 13.300, 예상가격: 13.945\n",
      "실제가격: 23.900, 예상가격: 22.894\n",
      "실제가격: 20.000, 예상가격: 17.875\n",
      "실제가격: 19.800, 예상가격: 21.379\n",
      "실제가격: 13.800, 예상가격: 17.633\n",
      "실제가격: 16.500, 예상가격: 24.963\n",
      "실제가격: 21.600, 예상가격: 24.789\n",
      "실제가격: 20.300, 예상가격: 14.752\n",
      "실제가격: 17.000, 예상가격: 22.204\n",
      "실제가격: 11.800, 예상가격: 12.039\n",
      "실제가격: 27.500, 예상가격: 25.296\n",
      "실제가격: 15.600, 예상가격: 14.811\n",
      "실제가격: 23.100, 예상가격: 17.146\n",
      "실제가격: 24.300, 예상가격: 20.329\n",
      "실제가격: 42.800, 예상가격: 30.419\n",
      "실제가격: 15.600, 예상가격: 10.481\n",
      "실제가격: 21.700, 예상가격: 18.061\n",
      "실제가격: 17.100, 예상가격: 18.119\n",
      "실제가격: 17.200, 예상가격: 15.590\n",
      "실제가격: 15.000, 예상가격: 18.787\n",
      "실제가격: 21.700, 예상가격: 19.256\n",
      "실제가격: 18.600, 예상가격: 23.619\n",
      "실제가격: 21.000, 예상가격: 18.771\n",
      "실제가격: 33.100, 예상가격: 30.499\n",
      "실제가격: 31.500, 예상가격: 30.219\n",
      "실제가격: 20.100, 예상가격: 18.468\n",
      "실제가격: 29.800, 예상가격: 30.913\n",
      "실제가격: 15.200, 예상가격: 20.110\n",
      "실제가격: 15.000, 예상가격: 14.729\n",
      "실제가격: 27.500, 예상가격: 17.340\n",
      "실제가격: 22.600, 예상가격: 21.403\n",
      "실제가격: 20.000, 예상가격: 21.268\n",
      "실제가격: 21.400, 예상가격: 23.592\n",
      "실제가격: 23.500, 예상가격: 31.816\n",
      "실제가격: 31.200, 예상가격: 30.416\n",
      "실제가격: 23.700, 예상가격: 26.630\n",
      "실제가격: 7.400, 예상가격: 4.750\n",
      "실제가격: 48.300, 예상가격: 30.247\n",
      "실제가격: 24.400, 예상가격: 19.920\n",
      "실제가격: 22.600, 예상가격: 23.396\n",
      "실제가격: 18.300, 예상가격: 21.424\n",
      "실제가격: 23.300, 예상가격: 23.848\n",
      "실제가격: 17.100, 예상가격: 23.357\n",
      "실제가격: 27.900, 예상가격: 23.270\n",
      "실제가격: 44.800, 예상가격: 31.267\n",
      "실제가격: 50.000, 예상가격: 31.918\n",
      "실제가격: 23.000, 예상가격: 24.690\n",
      "실제가격: 21.400, 예상가격: 20.855\n",
      "실제가격: 10.200, 예상가격: 14.993\n",
      "실제가격: 23.300, 예상가격: 31.104\n",
      "실제가격: 23.200, 예상가격: 17.434\n",
      "실제가격: 18.900, 예상가격: 21.091\n",
      "실제가격: 13.400, 예상가격: 13.548\n",
      "실제가격: 21.900, 예상가격: 25.065\n",
      "실제가격: 24.800, 예상가격: 31.102\n",
      "실제가격: 11.900, 예상가격: 22.600\n",
      "실제가격: 24.300, 예상가격: 21.277\n",
      "실제가격: 13.800, 예상가격: 0.534\n",
      "실제가격: 24.700, 예상가격: 27.244\n",
      "실제가격: 14.100, 예상가격: 17.039\n",
      "실제가격: 18.700, 예상가격: 17.856\n",
      "실제가격: 28.100, 예상가격: 21.841\n",
      "실제가격: 19.800, 예상가격: 23.793\n",
      "실제가격: 26.700, 예상가격: 28.095\n",
      "실제가격: 21.700, 예상가격: 20.632\n",
      "실제가격: 22.000, 예상가격: 21.376\n",
      "실제가격: 22.900, 예상가격: 19.696\n",
      "실제가격: 10.400, 예상가격: 7.472\n",
      "실제가격: 21.900, 예상가격: 20.883\n",
      "실제가격: 20.600, 예상가격: 21.646\n",
      "실제가격: 26.400, 예상가격: 26.256\n",
      "실제가격: 41.300, 예상가격: 33.194\n",
      "실제가격: 17.200, 예상가격: 11.916\n",
      "실제가격: 27.100, 예상가격: 16.263\n",
      "실제가격: 20.400, 예상가격: 16.504\n",
      "실제가격: 16.500, 예상가격: 6.624\n",
      "실제가격: 24.400, 예상가격: 18.643\n",
      "실제가격: 8.400, 예상가격: 3.210\n",
      "실제가격: 23.000, 예상가격: 24.945\n",
      "실제가격: 9.700, 예상가격: 11.605\n",
      "실제가격: 50.000, 예상가격: 46.204\n",
      "실제가격: 30.500, 예상가격: 30.492\n",
      "실제가격: 12.300, 예상가격: 13.876\n",
      "실제가격: 19.400, 예상가격: 16.748\n",
      "실제가격: 21.200, 예상가격: 23.044\n",
      "실제가격: 20.300, 예상가격: 21.185\n",
      "실제가격: 18.800, 예상가격: 18.979\n",
      "실제가격: 33.400, 예상가격: 36.073\n",
      "실제가격: 18.500, 예상가격: 12.451\n",
      "실제가격: 19.600, 예상가격: 20.646\n",
      "실제가격: 33.200, 예상가격: 30.532\n",
      "실제가격: 13.100, 예상가격: 20.616\n",
      "실제가격: 7.500, 예상가격: 11.070\n",
      "실제가격: 13.600, 예상가격: 21.752\n",
      "실제가격: 17.400, 예상가격: 16.426\n",
      "실제가격: 8.400, 예상가격: 12.386\n",
      "실제가격: 35.400, 예상가격: 35.718\n",
      "실제가격: 24.000, 예상가격: 20.860\n",
      "실제가격: 13.400, 예상가격: 18.165\n",
      "실제가격: 26.200, 예상가격: 25.640\n",
      "실제가격: 7.200, 예상가격: 8.065\n",
      "실제가격: 13.100, 예상가격: 13.538\n",
      "실제가격: 24.500, 예상가격: 21.769\n",
      "실제가격: 37.200, 예상가격: 34.013\n",
      "실제가격: 25.000, 예상가격: 26.278\n",
      "실제가격: 24.100, 예상가격: 25.525\n",
      "실제가격: 16.600, 예상가격: 14.515\n",
      "실제가격: 32.900, 예상가격: 31.906\n",
      "실제가격: 36.200, 예상가격: 27.315\n",
      "실제가격: 11.000, 예상가격: 13.405\n",
      "실제가격: 7.200, 예상가격: 7.483\n",
      "실제가격: 22.800, 예상가격: 26.050\n",
      "실제가격: 28.700, 예상가격: 26.553\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(Y_test)):\n",
    "    label = Y_test[i]\n",
    "    prediction = Y_prediction[i]\n",
    "    print(\"실제가격: {:.3f}, 예상가격: {:.3f}\".format(label, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "057e13bc-322a-4fc3-9edb-200e5eca46d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dad4cf0-8726-46f4-91f4-dbb9b448860b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
