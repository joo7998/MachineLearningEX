{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52c7818f-1993-4ce8-9dd3-b3c888190bba",
   "metadata": {},
   "source": [
    "# Natural Lang Processing(NLP) 자연어처리 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f54dff9-af31-4903-ac03-55d5e86b7a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6939f125-4699-4338-9c95-4532c1b0d635",
   "metadata": {},
   "source": [
    "# 1. Tokenization of Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63109a98-110e-46ea-8220-d12611083119",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "306a58b4-ce6e-48f8-9532-695716a2d783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['해보지', '않으면', '해낼', '수', '없다']\n"
     ]
    }
   ],
   "source": [
    "text = '해보지 않으면 해낼 수 없다'\n",
    "result = text_to_word_sequence(text)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ea5de2-e1be-47b4-98a9-a033bb86800b",
   "metadata": {},
   "source": [
    "# Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e3612341-3cb6-49ab-8c3a-23b0b6914c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 빈도수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbc3c6e3-f996-4cc5-90a4-4e4213efdcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3f3135a-4ba4-49f5-829f-e4978a48ee60",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['먼저 텍스트의 각 단어를 나누어 토큰화합니다.', \n",
    "        '텍스트의 단어로 토큰화해야 딥러닝에서 인식됩니다.', \n",
    "        '토큰화한 결과는 딥러닝에서 사용할 수 있습니다.',]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96046c42-4c60-459d-8af5-2a87a7addbe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('먼저', 1), ('텍스트의', 2), ('각', 1), ('단어를', 1), ('나누어', 1), ('토큰화합니다', 1), ('단어로', 1), ('토큰화해야', 1), ('딥러닝에서', 2), ('인식됩니다', 1), ('토큰화한', 1), ('결과는', 1), ('사용할', 1), ('수', 1), ('있습니다', 1)])\n"
     ]
    }
   ],
   "source": [
    "token = Tokenizer()        # 토큰화 함수 지정\n",
    "token.fit_on_texts(docs)   # 토큰화 함수에 문장 적용\n",
    "print(token.word_counts)   # 단어의 빈도 수를 계산한 결과 출력\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bb770b14-3a21-4621-a224-fb4207bc4b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "print(token.document_count) # 문장의 개수 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "855dd3cc-6c62-4fff-ab55-7633b6987a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {'단어를': 1, '텍스트의': 2, '각': 1, '토큰화합니다': 1, '먼저': 1, '나누어': 1, '토큰화해야': 1, '단어로': 1, '인식됩니다': 1, '딥러닝에서': 2, '토큰화한': 1, '수': 1, '결과는': 1, '사용할': 1, '있습니다': 1})\n"
     ]
    }
   ],
   "source": [
    "print(token.word_docs)      \n",
    "# 각 단어들이 몇개의 문장에 나오는지"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "029e1044-9728-4b76-8755-c17cef07234b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'텍스트의': 1, '딥러닝에서': 2, '먼저': 3, '각': 4, '단어를': 5, '나누어': 6, '토큰화합니다': 7, '단어로': 8, '토큰화해야': 9, '인식됩니다': 10, '토큰화한': 11, '결과는': 12, '사용할': 13, '수': 14, '있습니다': 15}\n"
     ]
    }
   ],
   "source": [
    "print(token.word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf2b138-41b8-4a5b-a7e9-bfe0652a57af",
   "metadata": {},
   "source": [
    "# 2. One Hot Encoding of a Word\n",
    "- 다중 분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a197630a-0766-4ecb-a3c9-b5bff754dcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e07b358c-7821-43a1-817f-ea5e05e24d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'오랜동안': 1, '꿈꾸는': 2, '이는': 3, '그': 4, '꿈을': 5, '닮아간다': 6}\n"
     ]
    }
   ],
   "source": [
    "text = '오랜동안 꿈꾸는 이는 그 꿈을 닮아간다'\n",
    "\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts([text])\n",
    "print(token.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6714688-abc0-46dd-854e-c806a649f6b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2, 3, 4, 5, 6]]\n"
     ]
    }
   ],
   "source": [
    "x = token.texts_to_sequences([text])\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5bb017fd-9a4b-4f6e-84e4-93ac27b2822e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[0. 1. 0. 0. 0. 0. 0.]\n",
      "  [0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 1. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0.]\n",
      "  [0. 0. 0. 0. 0. 0. 1.]]]\n"
     ]
    }
   ],
   "source": [
    "# 0 ~ 6 정수 index => 0~1 배열로 \n",
    "from keras.utils import to_categorical\n",
    "\n",
    "# 인덱스 수에 하나를 추가해서 원-핫 인코딩 배열 만들기\n",
    "word_size = len(token.word_index) +1\n",
    "x = to_categorical(x, num_classes=word_size)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "03b70097-b4d6-459f-8298-7557f3e440b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오랫동안 [0. 1. 0. 0. 0. 0. 0.]\n",
    "# 꿈구는   [0. 0. 1. 0. 0. 0. 0.]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373793de-b3ca-4847-b3ab-3db43f464877",
   "metadata": {},
   "source": [
    "# 3. Word Embedding\n",
    "- one hot encoding = 벡터의 길이가 너무 길어짐 (쓸데없는 공간, 계산)\n",
    "- 주어진 배열을 정해진 길이로 압축 \n",
    "- eg. 16차원 => 4차원 벡터\n",
    "- 오차 역전파 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "54f7d78d-d734-47ff-8101-c19eb98ee4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.models import Sequential\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(16,4))\n",
    "# 총 단어 수 16, 임베딩 후 출력되는 벡터 크기 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b4dfa9-3005-4bf7-925d-4565dad38817",
   "metadata": {},
   "source": [
    "# 4. Text 읽고 , 긍정 부정 예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6a8add9-8878-4404-ab96-b2d5dd6e584d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 리뷰 => class 지정 : 긍정(1) 부정(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0b78403b-99d5-4ca7-8179-bbbe60700a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = ['너무 재밌네요','최고예요',\n",
    "        '참 잘 만든 영화예요','추천하고 싶은 영화입니다.',\n",
    "        '한 번 더 보고싶네요','글쎄요','별로예요',\n",
    "        '생각보다 지루하네요','연기가 어색해요','재미없어요']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "57c0eaa0-d715-40b7-991d-acd15f396dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "classes = np.array([1,1,1,1,1,0,0,0,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f873073b-7050-4369-be15-c3b9b364cee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'너무': 1, '재밌네요': 2, '최고예요': 3, '참': 4, '잘': 5, '만든': 6, '영화예요': 7, '추천하고': 8, '싶은': 9, '영화입니다': 10, '한': 11, '번': 12, '더': 13, '보고싶네요': 14, '글쎄요': 15, '별로예요': 16, '생각보다': 17, '지루하네요': 18, '연기가': 19, '어색해요': 20, '재미없어요': 21}\n"
     ]
    }
   ],
   "source": [
    "# 토큰화\n",
    "token = Tokenizer()\n",
    "token.fit_on_texts(docs)\n",
    "print(token.word_index)   # 토큰화 된 결과를 출력해 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1983ba7b-ad69-4d3e-9433-d153e9ddc432",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2], [3], [4, 5, 6, 7], [8, 9, 10], [11, 12, 13, 14], [15], [16], [17, 18], [19, 20], [21]]\n"
     ]
    }
   ],
   "source": [
    "x = token.texts_to_sequences(docs)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23bb307d-0689-4282-a9d4-f366f65815bd",
   "metadata": {},
   "source": [
    "# Padding\n",
    "- 딥러닝 모델에 입력을 하려면 학습 데이터의 길이가 동일해야\n",
    "- 길이를 똑같이 맞춰 주는 작업\n",
    "- 원하는 길이보다 짧은 부분은 숫자 0을 넣어서 채워 \n",
    "- 긴 데이터는 잘라서 같은 길이 \n",
    "- 앞에서 생성한 x배열을 4개의 길이로"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2064bfbc-64fa-46bb-9faf-c92e3bcf8083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0  1  2]\n",
      " [ 0  0  0  3]\n",
      " [ 4  5  6  7]\n",
      " [ 0  8  9 10]\n",
      " [11 12 13 14]\n",
      " [ 0  0  0 15]\n",
      " [ 0  0  0 16]\n",
      " [ 0  0 17 18]\n",
      " [ 0  0 19 20]\n",
      " [ 0  0  0 21]]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "padded_x = pad_sequences(x, 4)  \n",
    "print(padded_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea69d75-bdd2-4c1d-be65-dbaf3c619a77",
   "metadata": {},
   "source": [
    "- 총 몇 개 index (입력), \n",
    "- 8개 embeddding 결과를 사용(출력), \n",
    "- padding 후, 4개 길이로 맞춰줌 => 매번 입력될 단어 수 4 (단어 수)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2ef9103-99ff-41cf-8b69-eccb1a3acfbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_size = len(token.word_index) +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3121d031-749c-496d-95f7-867f6461ea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 단어 임베딩을 포함 & 딥러닝 모델 \n",
    "\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(word_size, 8, input_length=4))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dc88870d-c98f-4400-84c2-d4a04ff7164c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 4, 8)              176       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 209\n",
      "Trainable params: 209\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "784c483f-043b-40f2-8530-6e95cfaf2da1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ac063ca-31c3-429b-afb2-d7fa77617cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6841 - accuracy: 0.8000\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6819 - accuracy: 0.9000\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6797 - accuracy: 0.9000\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6776 - accuracy: 0.9000\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6754 - accuracy: 0.9000\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6732 - accuracy: 0.9000\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6710 - accuracy: 0.9000\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6688 - accuracy: 0.9000\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 1000us/step - loss: 0.6666 - accuracy: 1.0000\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6644 - accuracy: 1.0000\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6622 - accuracy: 1.0000\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 999us/step - loss: 0.6600 - accuracy: 1.0000\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 0s/step - loss: 0.6578 - accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6556 - accuracy: 1.0000\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6534 - accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 998us/step - loss: 0.6511 - accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6489 - accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 997us/step - loss: 0.6466 - accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6444 - accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 993us/step - loss: 0.6421 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1c0ce6a4748>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(padded_x, classes, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c3051865-5115-4eca-a355-b16de24c6621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 1ms/step - loss: 0.6398 - accuracy: 1.0000\n",
      "\n",
      " Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n Accuracy: %.4f\" % (model.evaluate(padded_x, classes)[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebad1447-c7ec-402f-bb5c-925d4db18266",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
